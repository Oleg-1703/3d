#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è 3D Gaussian Splatting —Å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python3 automated_3dgs_pipeline.py --dataset data/dataset512 --max_size 1980 --prompt central_object
"""

import os
import sys
import argparse
import subprocess
import shutil
from pathlib import Path
import json
import time


class Colors:
    """ANSI color codes for terminal output"""
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'


def print_step(step_num, title, message=""):
    """Print formatted step information"""
    print(f"\n{Colors.CYAN}{'='*60}{Colors.ENDC}")
    print(f"{Colors.BOLD}{Colors.BLUE}–®–∞–≥ {step_num}: {title}{Colors.ENDC}")
    if message:
        print(f"{Colors.WHITE}{message}{Colors.ENDC}")
    print(f"{Colors.CYAN}{'='*60}{Colors.ENDC}")


def print_success(message):
    """Print success message"""
    print(f"{Colors.GREEN}‚úì {message}{Colors.ENDC}")


def print_error(message):
    """Print error message"""
    print(f"{Colors.RED}‚ùå {message}{Colors.ENDC}")


def print_warning(message):
    """Print warning message"""
    print(f"{Colors.YELLOW}‚ö†Ô∏è  {message}{Colors.ENDC}")


def run_command(command, description, check_success=True, cwd=None):
    """Run shell command with error handling"""
    print(f"{Colors.WHITE}–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è: {description}{Colors.ENDC}")
    print(f"{Colors.CYAN}–ö–æ–º–∞–Ω–¥–∞: {command}{Colors.ENDC}")
    
    try:
        result = subprocess.run(
            command, 
            shell=True, 
            capture_output=True, 
            text=True, 
            cwd=cwd
        )
        
        if result.returncode == 0:
            print_success(f"{description} –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
            if result.stdout.strip():
                print(f"–í—ã–≤–æ–¥: {result.stdout.strip()}")
            return True
        else:
            print_error(f"{description} –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–æ–π (–∫–æ–¥: {result.returncode})")
            if result.stderr.strip():
                print(f"–û—à–∏–±–∫–∞: {result.stderr.strip()}")
            if result.stdout.strip():
                print(f"–í—ã–≤–æ–¥: {result.stdout.strip()}")
            if check_success:
                return False
            return True
            
    except Exception as e:
        print_error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ {description}: {e}")
        return False


def validate_environment():
    """Validate required environment and dependencies"""
    print_step(0, "–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    
    # Check Python packages
    required_packages = ["torch", "torchvision", "cv2", "numpy", "PIL"]
    for package in required_packages:
        try:
            __import__(package)
            print_success(f"–ü–∞–∫–µ—Ç {package} –Ω–∞–π–¥–µ–Ω")
        except ImportError:
            print_error(f"–ü–∞–∫–µ—Ç {package} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
    
    # Check CUDA
    try:
        import torch
        if torch.cuda.is_available():
            print_success(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name()}")
        else:
            print_warning("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU")
    except:
        print_warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å CUDA")
    
    # Check required scripts and directories
    required_files = [
        "convert.py",
        "train.py", 
        "render.py",
        "resize_images.py",
        "Tracking-Anything-with-DEVA/demo/demo_with_text.py"
    ]
    
    for file_path in required_files:
        if os.path.exists(file_path):
            print_success(f"–§–∞–π–ª –Ω–∞–π–¥–µ–Ω: {file_path}")
        else:
            print_error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return False
    
    return True


def setup_dataset_structure(dataset_path):
    """Setup proper dataset folder structure"""
    print_step(1, "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞", f"–ü—É—Ç—å: {dataset_path}")
    
    dataset_path = Path(dataset_path)
    images_dir = dataset_path / "images"
    input_dir = dataset_path / "input"
    
    # Check if images folder exists
    if not images_dir.exists():
        print_error(f"–ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {images_dir}")
        return False
    
    # Create input symlink for COLMAP (COLMAP expects 'input' folder)
    if input_dir.exists():
        if input_dir.is_symlink():
            input_dir.unlink()
        elif input_dir.is_dir():
            shutil.rmtree(input_dir)
    
    input_dir.symlink_to("images", target_is_directory=True)
    print_success(f"–°–æ–∑–¥–∞–Ω–∞ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∞—è —Å—Å—ã–ª–∫–∞: input -> images")
    
    # Create other required directories
    required_dirs = ["distorted", "sparse", "output_sam"]
    for dir_name in required_dirs:
        dir_path = dataset_path / dir_name
        dir_path.mkdir(exist_ok=True)
        print_success(f"–°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {dir_name}")
    
    return True


def resize_images(dataset_path, max_size):
    """Resize images if max_size is specified"""
    if max_size is None:
        print_warning("–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è (max_size –Ω–µ —É–∫–∞–∑–∞–Ω)")
        return True
    
    print_step(2, "–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max_size}")
    
    command = f"python3 resize_images.py {dataset_path} --max_size={max_size}"
    return run_command(command, "–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")


def run_colmap(dataset_path):
    """Run COLMAP for structure-from-motion"""
    print_step(3, "–ó–∞–ø—É—Å–∫ COLMAP", "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ 3D –º–æ–¥–µ–ª–∏")
    
    # Set environment variables for headless operation
    env_commands = [
        "export QT_QPA_PLATFORM=offscreen",
        "export DISPLAY=:99"
    ]
    
    for env_cmd in env_commands:
        run_command(env_cmd, f"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è: {env_cmd}", check_success=False)
    
    # Run COLMAP conversion
    command = f"xvfb-run -a python3 convert.py -s {dataset_path}"
    success = run_command(command, "–û–±—Ä–∞–±–æ—Ç–∫–∞ COLMAP")
    
    if not success:
        print_error("COLMAP –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")
        return False
    
    # Move sparse data to correct location
    source_sparse = Path(dataset_path) / "distorted" / "sparse"
    target_sparse = Path(dataset_path) / "sparse"
    
    if source_sparse.exists():
        if target_sparse.exists():
            shutil.rmtree(target_sparse)
        shutil.move(str(source_sparse), str(target_sparse))
        print_success("–î–∞–Ω–Ω—ã–µ COLMAP –ø–µ—Ä–µ–º–µ—â–µ–Ω—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é")
    else:
        print_error(f"–î–∞–Ω–Ω—ã–µ COLMAP –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {source_sparse}")
        return False
    
    return True


def run_segmentation(dataset_path, prompt):
    """Run DEVA segmentation"""
    print_step(4, "–ó–∞–ø—É—Å–∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", f"–ü—Ä–æ–º–ø—Ç: {prompt}")
    
    # Change to DEVA directory
    deva_dir = "Tracking-Anything-with-DEVA"
    if not os.path.exists(deva_dir):
        print_error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è DEVA –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {deva_dir}")
        return False
    
    # Prepare paths
    images_path = f"../{dataset_path}/images"
    output_path = f"../{dataset_path}/output_sam"
    
    # Run segmentation
    command = (f"python3 demo/demo_with_text.py "
               f"--chunk_size 4 "
               f"--img_path {images_path} "
               f"--amp "
               f"--temporal_setting semionline "
               f"--size 480 "
               f"--output {output_path} "
               f"--prompt {prompt}")
    
    success = run_command(command, "–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é DEVA", cwd=deva_dir)
    
    if not success:
        print_error("–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π")
        return False
    
    # Move annotation files to object_mask folder
    os.chdir("..")  # Return to main directory
    
    source_annotation = Path(dataset_path) / "output_sam" / "Annotation"
    target_mask = Path(dataset_path) / "object_mask"
    
    if source_annotation.exists():
        if target_mask.exists():
            shutil.rmtree(target_mask)
        shutil.move(str(source_annotation), str(target_mask))
        print_success("–ú–∞—Å–∫–∏ –ø–µ—Ä–µ–º–µ—â–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é object_mask")
    else:
        print_error(f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {source_annotation}")
        return False
    
    return True


def run_training(dataset_path):
    """Run 3D Gaussian Splatting training"""
    print_step(5, "–ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ 3DGS", "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Gaussian Splatting")
    
    output_path = f"output/{Path(dataset_path).name}"
    config_path = "config/gaussian_dataset/train.json"
    
    # Check if config exists
    if not os.path.exists(config_path):
        print_error(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
        return False
    
    command = (f"python3 train.py "
               f"-s {dataset_path} "
               f"-r 1 "
               f"-m {output_path} "
               f"--config_file {config_path}")
    
    return run_command(command, "–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ 3DGS")


def run_rendering(dataset_path):
    """Run rendering with trained model"""
    print_step(6, "–ó–∞–ø—É—Å–∫ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞", "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    output_path = f"output/{Path(dataset_path).name}"
    
    command = f"python3 render.py -m {output_path} --num_classes 2"
    return run_command(command, "–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")


def cleanup_intermediate_files(dataset_path):
    """Clean up intermediate files to save space"""
    print_step(7, "–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
    
    dataset_path = Path(dataset_path)
    cleanup_dirs = ["distorted", "output_sam", "input"]
    
    for dir_name in cleanup_dirs:
        dir_path = dataset_path / dir_name
        if dir_path.exists():
            if dir_path.is_symlink():
                dir_path.unlink()
            else:
                shutil.rmtree(dir_path)
            print_success(f"–£–¥–∞–ª–µ–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {dir_name}")


def generate_summary_report(dataset_path, start_time):
    """Generate summary report"""
    end_time = time.time()
    duration = end_time - start_time
    
    print_step(8, "–û—Ç—á–µ—Ç –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏")
    
    dataset_path = Path(dataset_path)
    output_path = Path("output") / dataset_path.name
    
    print(f"{Colors.WHITE}–î–∞—Ç–∞—Å–µ—Ç: {Colors.CYAN}{dataset_path}{Colors.ENDC}")
    print(f"{Colors.WHITE}–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {Colors.CYAN}{duration:.2f} —Å–µ–∫—É–Ω–¥{Colors.ENDC}")
    print(f"{Colors.WHITE}–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {Colors.CYAN}{output_path}{Colors.ENDC}")
    
    # Check output files
    if output_path.exists():
        train_dir = output_path / "train"
        test_dir = output_path / "test"
        
        if train_dir.exists():
            train_files = list(train_dir.glob("*.png"))
            print_success(f"–°–æ–∑–¥–∞–Ω–æ {len(train_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞")
        
        if test_dir.exists():
            test_files = list(test_dir.glob("*.png"))
            print_success(f"–°–æ–∑–¥–∞–Ω–æ {len(test_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞")
    
    print(f"\n{Colors.GREEN}{'='*60}")
    print(f"üéâ –ü–ê–ô–ü–õ–ê–ô–ù –í–´–ü–û–õ–ù–ï–ù –£–°–ü–ï–®–ù–û! üéâ")
    print(f"{'='*60}{Colors.ENDC}")


def main():
    """Main pipeline function"""
    parser = argparse.ArgumentParser(
        description="–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è 3D Gaussian Splatting —Å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python3 automated_3dgs_pipeline.py --dataset data/dataset512 --prompt central_object
  python3 automated_3dgs_pipeline.py --dataset data/my_scene --max_size 1920 --prompt "red car"
  python3 automated_3dgs_pipeline.py --dataset data/room --max_size 1080 --prompt "sofa" --no_cleanup
        """
    )
    
    parser.add_argument(
        "--dataset", 
        required=True, 
        help="–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É (–¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–∞–ø–∫—É 'images')"
    )
    
    parser.add_argument(
        "--max_size", 
        type=int, 
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ä–µ—Å–∞–π–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1980)"
    )
    
    parser.add_argument(
        "--prompt", 
        default="central_object", 
        help="–ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: central_object)"
    )
    
    parser.add_argument(
        "--no_cleanup", 
        action="store_true", 
        help="–ù–µ —É–¥–∞–ª—è—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"
    )
    
    parser.add_argument(
        "--skip_colmap", 
        action="store_true", 
        help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —ç—Ç–∞–ø COLMAP (–µ—Å–ª–∏ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω)"
    )
    
    parser.add_argument(
        "--skip_segmentation", 
        action="store_true", 
        help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —ç—Ç–∞–ø —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (–µ—Å–ª–∏ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω)"
    )
    
    args = parser.parse_args()
    
    # Start timing
    start_time = time.time()
    
    print(f"{Colors.MAGENTA}{'='*60}")
    print(f"üöÄ –ó–ê–ü–£–°–ö –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê 3DGS üöÄ")
    print(f"{'='*60}{Colors.ENDC}")
    
    # Validate environment
    if not validate_environment():
        print_error("–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞")
        sys.exit(1)
    
    # Setup dataset structure
    if not setup_dataset_structure(args.dataset):
        print_error("–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞")
        sys.exit(1)
    
    # Resize images if needed
    if not resize_images(args.dataset, args.max_size):
        print_error("–û—à–∏–±–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        sys.exit(1)
    
    # Run COLMAP
    if not args.skip_colmap:
        if not run_colmap(args.dataset):
            print_error("–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è COLMAP")
            sys.exit(1)
    else:
        print_warning("COLMAP –ø—Ä–æ–ø—É—â–µ–Ω")
    
    # Run segmentation
    if not args.skip_segmentation:
        if not run_segmentation(args.dataset, args.prompt):
            print_error("–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
            sys.exit(1)
    else:
        print_warning("–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    
    # Run training
    if not run_training(args.dataset):
        print_error("–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏")
        sys.exit(1)
    
    # Run rendering
    if not run_rendering(args.dataset):
        print_error("–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞")
        sys.exit(1)
    
    # Cleanup
    if not args.no_cleanup:
        cleanup_intermediate_files(args.dataset)
    else:
        print_warning("–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    
    # Generate summary
    generate_summary_report(args.dataset, start_time)


if __name__ == "__main__":
    main()